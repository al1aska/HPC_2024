{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yaf7k3rbCmpB",
        "outputId": "a92fc15d-f473-499b-8d2e-4ee39bb3933c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-x90m1fwh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-x90m1fwh\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10733 sha256=25d7336482e00933626da974c4579936e1bcd2169b147b54a7d099795d56a4e1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x56hdm_5/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnVKEEidCnfV",
        "outputId": "e3741c5d-adea-4844-c9a3-b3d00ed98ad0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your CUDA code to a file\n",
        "%%writefile hello_cuda.cu\n",
        "#include <iostream>\n",
        "__global__ void hello() {\n",
        "    printf(\"Hello from CUDA kernel!\\n\");\n",
        "}\n",
        "int main() {\n",
        "    hello<<<1, 10>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1tt8ipdCp39",
        "outputId": "284fd5ce-10ef-47d8-b6dc-c56068f11923"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Компилируем код\n",
        "!nvcc hello_cuda.cu -o hello_cuda\n",
        "\n",
        "# Запускаем исполняемый файл\n",
        "!./hello_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXjTjwO4Cp1T",
        "outputId": "70873c8a-bd72-430a-f173-567bd15488c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from CUDA kernel!\n",
            "Hello from CUDA kernel!\n",
            "Hello from CUDA kernel!\n",
            "Hello from CUDA kernel!\n",
            "Hello from CUDA kernel!\n",
            "Hello from CUDA kernel!\n",
            "Hello from CUDA kernel!\n",
            "Hello from CUDA kernel!\n",
            "Hello from CUDA kernel!\n",
            "Hello from CUDA kernel!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kernel.cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <string>\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <time.h>\n",
        "#define BLOCK_SIZE 32\n",
        "\n",
        "// объявление функций\n",
        "\n",
        "// функция заполнения матриц рандомными числами\n",
        "void filling_matrices(float* matrix_1, float* matrix_2, const int matrix_size);\n",
        "\n",
        "// перемножение матриц на графическом процессоре\n",
        "__global__ void matrix_multiplication_GPU(float* matrix_1, float* matrix_2, float* multiplication_result, const int matrix_size);\n",
        "\n",
        "// перемножение матриц на центральном процессоре\n",
        "float matrix_multiplication_CPU(float* matrix_1, float* matrix_2, float* multiplication_result, const int matrix_size);\n",
        "\n",
        "// проверка результата перемножения: равно ли перемножение матриц на гпу перемножению матриц на цпу\n",
        "bool is_equal(float* matrix_1, float* matrix_2, const int matrix_size);\n",
        "\n",
        "\n",
        "int main(int argc, char* argv[])\n",
        "{\n",
        "\t// Matrix size is matrix_size * matrix_size.\n",
        "\tconst int matrix_size = 512;\n",
        "\tstd::cout << \"размер матриц: \" << matrix_size << std::endl;\n",
        "\n",
        "\t// выделение памяти под матрицы\n",
        "\tfloat* matrix_1 = new float[matrix_size * matrix_size];\n",
        "\tfloat* matrix_2 = new float[matrix_size * matrix_size];\n",
        "\tfloat* cpumultiplication_result = new float[matrix_size * matrix_size];\n",
        "\tfloat* gpumultiplication_result = new float[matrix_size * matrix_size];\n",
        "\n",
        "\t// заполнение матриц\n",
        "\tfilling_matrices(matrix_1, matrix_2, matrix_size);\n",
        "\n",
        "\t// выделение памяти на гпу\n",
        "\tfloat* cudamatrix_1, * cudamatrix_2, * cudamultiplication_result;\n",
        "\tcudaMalloc((void**)&cudamatrix_1, matrix_size * matrix_size * sizeof(float));\n",
        "\tcudaMalloc((void**)&cudamatrix_2, matrix_size * matrix_size * sizeof(float));\n",
        "\tcudaMalloc((void**)&cudamultiplication_result, matrix_size * matrix_size * sizeof(float));\n",
        "\n",
        "\t// объявление событий для замера времени\n",
        "\tcudaEvent_t start, stop;\n",
        "\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&stop);\n",
        "\n",
        "\t// количество нитей и блоков\n",
        "\tdim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n",
        "\tdim3 blocks(matrix_size / threads.x, matrix_size / threads.y);\n",
        "\n",
        "\t// начало записываемого события (замера времени)\n",
        "\tcudaEventRecord(start, 0);\n",
        "\n",
        "\t// копирование матриц на гпу\n",
        "\tcudaMemcpy(cudamatrix_1, matrix_1, matrix_size * matrix_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\tcudaMemcpy(cudamatrix_2, matrix_2, matrix_size * matrix_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "\t// произведение матриц на гпу\n",
        "\tmatrix_multiplication_GPU <<< blocks, threads >>> (cudamatrix_1, cudamatrix_2, cudamultiplication_result, matrix_size);\n",
        "\n",
        "\t// копирование результата обратно на хост\n",
        "\tcudaMemcpy(gpumultiplication_result, cudamultiplication_result, matrix_size * matrix_size * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "\t// окончание записи события (замера времени)\n",
        "\tcudaEventRecord(stop, 0);\n",
        "\n",
        "\tcudaEventSynchronize(stop);\n",
        "\n",
        "\tfloat gpuTime = 0.0f;\n",
        "\tcudaEventElapsedTime(&gpuTime, start, stop);\n",
        "\n",
        "\tstd::cout << \"\\nвремя на GPU \" << std::fixed << std::setprecision(2) << gpuTime << \" ms\\n\";\n",
        "\n",
        "\t// произведение матриц на цпу\n",
        "\tfloat cpuTime = matrix_multiplication_CPU(matrix_1, matrix_2, cpumultiplication_result, matrix_size);\n",
        "\tstd::cout << \"\\nвремя на CPU \" << cpuTime << \" ms\\n\";\n",
        "\n",
        "\tstd::cout << \"\\nматрицы одинаковые?: \";\n",
        "\tis_equal(cpumultiplication_result, gpumultiplication_result, matrix_size) ? std::cout << \"да\" : std::cout << \"нет\";\n",
        "\n",
        "\tcudaFreeHost(matrix_1);\n",
        "\tcudaFreeHost(matrix_2);\n",
        "\tcudaFreeHost(cpumultiplication_result);\n",
        "\tcudaFreeHost(gpumultiplication_result);\n",
        "\tcudaEventDestroy(start);\n",
        "\tcudaEventDestroy(stop);\n",
        "\tcudaFree(cudamatrix_1);\n",
        "\tcudaFree(cudamatrix_2);\n",
        "\tcudaFree(cudamultiplication_result);\n",
        "\n",
        "\treturn 0;\n",
        "}\n",
        "\n",
        "void filling_matrices(float* matrix_1, float* matrix_2, const int matrix_size)\n",
        "{\n",
        "\tsrand(time(0));\n",
        "\tfloat* ptr_1 = matrix_1;\n",
        "\tfloat* ptr_2 = matrix_2;\n",
        "\tfor (int i = 0; i < matrix_size; i++)\n",
        "\t{\n",
        "\t\tfor (int j = 0; j < matrix_size; j++)\n",
        "\t\t{\n",
        "\t\t\t//*(ptr_1 + i * matrix_size + j) = rand() % 1000;\n",
        "\t\t\t//*(ptr_2 + i * matrix_size + j) = rand() % 1000;\n",
        "\n",
        "\t\t\t*(ptr_1) = rand() % 1000;\n",
        "\t\t\t*(ptr_2) = rand() % 1000;\n",
        "\t\t\tptr_1++;\n",
        "\t\t\tptr_2++;\n",
        "\t\t}\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void matrix_multiplication_GPU(float* matrix_1, float* matrix_2, float* multiplication_result, const int matrix_size)\n",
        "{\n",
        "\tint tx = threadIdx.x;\n",
        "\tint ty = threadIdx.y;\n",
        "\n",
        "\tfloat sum = 0.0f;\n",
        "\tint indexmatrix_1 = matrix_size * blockDim.y * blockIdx.y + matrix_size * threadIdx.y;\n",
        "\tint indexmatrix_2 = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "\tint indexmultiplication_result = matrix_size * BLOCK_SIZE * blockIdx.y + BLOCK_SIZE * blockIdx.x;\n",
        "\n",
        "\t// вычисление элемента\n",
        "\tfor (int k = 0; k < matrix_size; k++)\n",
        "\t{\n",
        "\t\tsum += matrix_1[indexmatrix_1 + k] * matrix_2[indexmatrix_2 + k * matrix_size];\n",
        "\t}\n",
        "\tmultiplication_result[indexmultiplication_result + matrix_size * ty + tx] = sum;\n",
        "}\n",
        "\n",
        "\n",
        "float matrix_multiplication_CPU(float* matrix_1, float* matrix_2, float* multiplication_result, const int matrix_size)\n",
        "{\n",
        "\tclock_t start = clock();\n",
        "\tfor (int i = 0; i < matrix_size; i++)\n",
        "\t{\n",
        "\t\tfor (int j = 0; j < matrix_size; j++)\n",
        "\t\t{\n",
        "\t\t\tmultiplication_result[i * matrix_size + j] = 0;\n",
        "\t\t\tfor (int k = 0; k < matrix_size; k++)\n",
        "\t\t\t{\n",
        "\t\t\t\tmultiplication_result[i * matrix_size + j] += matrix_1[i * matrix_size + k] * matrix_2[k * matrix_size + j];\n",
        "\t\t\t}\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tclock_t end = clock();\n",
        "\t// перевод времени в млс\n",
        "\tfloat time = ((float)(end - start) / CLOCKS_PER_SEC) * 1000;\n",
        "\n",
        "\treturn time;\n",
        "}\n",
        "\n",
        "\n",
        "bool is_equal(float* matrix_1, float* matrix_2, const int matrix_size)\n",
        "{\n",
        "\tint numBytes = matrix_size * matrix_size * sizeof(float);\n",
        "\treturn memcmp(matrix_1, matrix_2, numBytes) == 0; // memcmp из string сравнивает два блока памяти и возвращает 0 если они одинаковые\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovcY7A-GCpyD",
        "outputId": "b43048d4-06e2-400e-bda8-2b51966daa02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Компилируем код\n",
        "!nvcc kernel.cu -o kernel\n",
        "\n",
        "# Запускаем исполняемый файл\n",
        "!./kernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XMseMm_Drp6",
        "outputId": "2838a4ff-f41f-4b25-a991-a665ba370c1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "размер матриц: 512\n",
            "\n",
            "время на GPU 79.72 ms\n",
            "\n",
            "время на CPU 1958.30 ms\n",
            "\n",
            "матрицы одинаковые?: да"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "128 - 10.96/0.39\n",
        "\n",
        "256 - 84.24/0.71\n",
        "\n",
        "512 - 1808.94/2.84\n",
        "\n",
        "1024 - 11830.79/11.90\n",
        "\n",
        "2048 - 139993.84/72.23"
      ],
      "metadata": {
        "id": "yRbtZlwSBmYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install altair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mkevZb3CAky",
        "outputId": "ba0ef5f0-b522-4f07-f6f0-29b7a4ee7a54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (4.2.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair) (4.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.10/dist-packages (from altair) (2.2.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair) (0.12.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.18->altair) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import altair as alt\n",
        "import pandas as pd\n",
        "\n",
        "# Данные\n",
        "data = {\n",
        "    'Size': [128, 256, 512, 1024, 2048],\n",
        "    'CPU_Time': [10.96, 84.24, 1808.94, 11830.79, 139993.84],\n",
        "    'GPU_Time': [0.39, 0.71, 2.84, 11.90, 72.23]\n",
        "}\n",
        "\n",
        "# Создание DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Расчет ускорения\n",
        "df['Speedup'] = df['CPU_Time'] / df['GPU_Time']\n",
        "\n",
        "# Построение графика\n",
        "chart = alt.Chart(df).mark_line(point=True).encode(\n",
        "    x=alt.X('Size:Q', title='Размерность матриц'),\n",
        "    y=alt.Y('Speedup:Q', title='Ускорение'),\n",
        ").properties(\n",
        "    title='Ускорение работы программы на GPU относительно CPU'\n",
        ")\n",
        "\n",
        "# Отображение графика\n",
        "chart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "a3cWFLwwCC-1",
        "outputId": "a8eea56b-63a5-41de-d2be-3f11217a4613"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-f11438b6f0b947b1a9ed3f47f3884612\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-f11438b6f0b947b1a9ed3f47f3884612\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-f11438b6f0b947b1a9ed3f47f3884612\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-c2a33050dc73edfa84cd79561116b498\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"x\": {\"field\": \"Size\", \"title\": \"\\u0420\\u0430\\u0437\\u043c\\u0435\\u0440\\u043d\\u043e\\u0441\\u0442\\u044c \\u043c\\u0430\\u0442\\u0440\\u0438\\u0446\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Speedup\", \"title\": \"\\u0423\\u0441\\u043a\\u043e\\u0440\\u0435\\u043d\\u0438\\u0435\", \"type\": \"quantitative\"}}, \"title\": \"\\u0423\\u0441\\u043a\\u043e\\u0440\\u0435\\u043d\\u0438\\u0435 \\u0440\\u0430\\u0431\\u043e\\u0442\\u044b \\u043f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u044b \\u043d\\u0430 GPU \\u043e\\u0442\\u043d\\u043e\\u0441\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e CPU\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-c2a33050dc73edfa84cd79561116b498\": [{\"Size\": 128, \"CPU_Time\": 10.96, \"GPU_Time\": 0.39, \"Speedup\": 28.102564102564102}, {\"Size\": 256, \"CPU_Time\": 84.24, \"GPU_Time\": 0.71, \"Speedup\": 118.64788732394366}, {\"Size\": 512, \"CPU_Time\": 1808.94, \"GPU_Time\": 2.84, \"Speedup\": 636.9507042253522}, {\"Size\": 1024, \"CPU_Time\": 11830.79, \"GPU_Time\": 11.9, \"Speedup\": 994.1840336134454}, {\"Size\": 2048, \"CPU_Time\": 139993.84, \"GPU_Time\": 72.23, \"Speedup\": 1938.1675204208775}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}